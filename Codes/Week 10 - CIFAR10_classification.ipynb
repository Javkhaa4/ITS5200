{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "965aba75",
   "metadata": {},
   "source": [
    "## CIFAR10\n",
    "\n",
    "* classification (10 classes)\n",
    "* 60,000 training images (50k training; 10k testing)\n",
    "* RGB\n",
    "* 32x32\n",
    "* airplane (0); car (1); etc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37d744f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## !pip install torchvision\n",
    "## !pip install tensorflow-gpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "857c4975",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import os \n",
    "\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7882654-3d46-4acb-935a-bcc5c4f14fc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1270585b-f02d-4cf0-99f1-ff91eae6e9cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa4ed0e8-68b3-4e59-9629-7474635ae8d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## import tensorflow as tf\n",
    "## print(\"GPU Available: \", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "420ec57e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data_path = 'data/cifar10data/'\n",
    "\n",
    "cifar10_training = datasets.CIFAR10(data_path, train=True, download=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4382258",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cifar10_test = datasets.CIFAR10(data_path, train= False, download = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44a53116",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset CIFAR10\n",
      "    Number of datapoints: 50000\n",
      "    Root location: data/cifar10data/\n",
      "    Split: Train\n"
     ]
    }
   ],
   "source": [
    "print (cifar10_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b576470f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset CIFAR10\n",
      "    Number of datapoints: 10000\n",
      "    Root location: data/cifar10data/\n",
      "    Split: Test\n"
     ]
    }
   ],
   "source": [
    "print (cifar10_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0958b69b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img_train, label = cifar10_training [42]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8c095c7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c44b4493",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAIAAAD8GO2jAAAHu0lEQVR4nCXTW3Nk11kG4Pc7rLV3d6sljUaemrFjnMQ4thOnIFSFmxSFoYq/BcVfg7twExeEFD7Edg6WR8OMpFYf9lrrO3DB8x8e+pd//te3ri4++eC902nsT+1/Pv/8+ulb29V0ebWJkEwM75SZNkY/IjGGlVKYOcL76BZxXI6v7+8AZhKijMj7u324//BHL1bTpJ9/+dX99TPxdnv7+v7uPqC3b/Zhtj2br66uV/Mc8N4GfBTx9VpEqPeT++hjJJw5y6xvv9iqKENUJQB755IFpYqb6Upxd3/3u+RxPCjG+uLi9f3+cDg8nk7f/um7H/zgxXLcf3d7e3d/f3V5/k+f/nJa67I8MvmqEosgQ5mKVkowkTAigyZBwIZPovqH77/dnJ2LeoaTnx7/9/H8/Gqa1ktbDvuHly8XpG3PdF6tN2upuj7sdiKjCGaZEMHEFExLMjGQHt0DARRRCdap6D/+w88EJJQqhVyGex+tFA1sklaiWUSUSyGl9M2m2ggKT8jwkZERniAKiKoIg5goVYqoJAXE9Ofv/uUY1r2BHQkWce/gGJ7dOjQYzC6zVMmAj3Xhs9V5d890ACQFTKpMxAzu3cxdRQXCKu5D79orEeYKFkoHIycVQUkl15kIRmZuzEmUp+OSMUhZKT2NiFnUza2FkBBBghkk7sKQkIQo1cwIIlJRUjCQ5mBNh8dgEDIKsTeDcOtxXuuKeMSwqMISlsIcSQwqIqAMZIefsiGTVfVJnDERRWIhUckMEJDkIK1T5ABLlTOl9e7xETlevznd3OwvrlZXl7OxWw4LZAEIiQyAiUEAkYApoOaRYQiIUKAYfO8nIaHY/P7Lm/3+KCwZhOSffPwxIH+6vfv8y2/Ot9uPPnj7ybVun0pIZlBhgWdJqiQqmpkAQ6FZ0zxBAIvzCIAX+f779rhv3/zh5avbl25mNl48f/H+xz+5ubnZHY5LX8br9uvdbrOe3//R9UefPAU0PBIgTYMHw9yLSiJ1SCNNAGCNDG/0+9/e37za3T22Yz+crPeliQjV6Yuv//hweDz1HhmegaRXD8v9bx5+9skzLWLB5u6ZHkEJMOARkRoZksjIY+5nnr/63Xf/8dm3dTPtD62NZQxj5nm1ftjv2p+/2T3ut+uz7fai9WE+IvtwGGi0pqpIINjNQM5FHJ4Z2lonouGdHFL4N599efvwsPbNbrcH4BHb9ZpYSbQPb62vVxLw/WlZlt3FRn75tx86EpkWHghBCUrmpExwIlnnuSrJcBYIg5+/c37z6s1hfwCwtCUymQjinplIIXt4c+vRp0mev3f9d7/66NnTbY8Boe4jkElgTTAFUWaCQzNpIABirkz5q08/vH7r8t/+/b+nebp7pJvbV7/4+bt/9Ysffv3Vndny4x//hXkiokdKTmeXfN92lESEQIpyYjCYlC2DichJYZwEIU5PzxiJ9z96/tl/fnNxtm2WpehPf/ru2+9c6ixff3V48d4V4BL65v70xRen9zpNosFE4AwvohamLMMsgST2dOUkjzCEMBVIpWo5Pv37vy7i82/Xu93D8+fXGHJ1vv2vx4fl3p882UaO68uzm40VrZwBuFDxJOuIxIIB4jFGKWWEKZQloSQgCDNHEpV33p3N2t+s19tzWc/VA0j/8IOnDw8topIurPLixfzy5eHsSbbWpjIJiw0PpMOUFeDTOAGp3RsRPNmsEVcJHdE9AMrA+ODjt+/bMTyJffOEb//s9fKemSQnXfvtt311ObvkghOBIPAIJrE0EQn3KkX78CRjUk3p1otkd4+IjCBFwyAJUSCozETs3WjepPmI6Kzz/g7lIsxDgpm48owEISQZjojUEZZAWJ9l6jGOdvr/86d2rFnnOrXWLEcic6E6b29vlrU/RiAyift3f5zq4XHaROXKycwjMkScqXqACGrZAVLl4OAEG9IpOGopw/owixxJmYkYTD5KaYeHRsqlFPA4eyLeN5GHqGqJaUpoRupwW2wgocN7Ip2YbAxvQgUSzEqicDCYqHiYQKIQT7HhKXImye5LlU3RRHJ3ikwVCQuHZZTMsZqrkysItVZK7qccvEjhqtUaMTDViQ2RVHUS40GjVB4UTFiV8zGOxHnoR5npbJqQHMbTVKydMjR0YpaRplf6VEioICScJhqVYTmzKPfeuWp6oFBQKhMzz1QqSYTOM3vYJDXAmckQR/blRK6iKVQ9oqSow4qwQihldO4t69lGoDkyB9dSgnGIRyGZ6opFyZMBIhnkcOJUzwFiC1vaYfGhMlXiWgSdqs6alqfeZp09BguhuCUSQZFF6NSbZ9S5qCgGeevdQ1MKhaeLSCInngJIpSJ0RSWYmAkBrIgZGokIO8QhU6hHLXJ43M3bszYOTBnBBsdpOvgRTmbxME7PNttOqes5Ee5hI0gZrggEJ3N2CxVNGKsqiNw9zJuFILvzsGgPOykIDubKKQZPSS3MQc94i6BGZKdodnQYPIiFAirF4riZziL8uCyp2vYPOtWpSkmL64vZRzghuQFZ54mYe7eqxdMtzGIwWFFPo7d+YhrunhlVarB37x6BQW702E4Nrda1e6p3ICHMh6VJApQjXavu2nGSeQxLygwnEotksuFOQkVoUJCKN3YubekEHc1nWgeljsmGM2Kt5/8HlP+YdNEO/dwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=32x32>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d4789d49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img_test, label = cifar10_test [2345]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13b4a3fa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eac13ef6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=32x32>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "084bb395",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AugMix', 'AutoAugment', 'AutoAugmentPolicy', 'CenterCrop', 'ColorJitter', 'Compose', 'ConvertImageDtype', 'ElasticTransform', 'FiveCrop', 'GaussianBlur', 'Grayscale', 'InterpolationMode', 'Lambda', 'LinearTransformation', 'Normalize', 'PILToTensor', 'Pad', 'RandAugment', 'RandomAdjustSharpness', 'RandomAffine', 'RandomApply', 'RandomAutocontrast', 'RandomChoice', 'RandomCrop', 'RandomEqualize', 'RandomErasing', 'RandomGrayscale', 'RandomHorizontalFlip', 'RandomInvert', 'RandomOrder', 'RandomPerspective', 'RandomPosterize', 'RandomResizedCrop', 'RandomRotation', 'RandomSolarize', 'RandomVerticalFlip', 'Resize', 'TenCrop', 'ToPILImage', 'ToTensor', 'TrivialAugmentWide', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '_pil_constants', '_presets', 'autoaugment', 'functional', 'functional_pil', 'functional_tensor', 'transforms']\n"
     ]
    }
   ],
   "source": [
    "print ( dir(transforms) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cc57136b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PIL.Image.Image image mode=RGB size=32x32 at 0x1FF46627A08>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print (img_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "42fe26b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 32, 32])\n",
      "tensor([[[0.4980, 0.5804, 0.7529,  ..., 0.4392, 0.5647, 0.6000],\n",
      "         [0.3490, 0.4353, 0.5922,  ..., 0.6314, 0.6510, 0.6392],\n",
      "         [0.3843, 0.3686, 0.4314,  ..., 0.6784, 0.6627, 0.6784],\n",
      "         ...,\n",
      "         [0.6118, 0.5922, 0.5843,  ..., 0.5882, 0.5843, 0.5569],\n",
      "         [0.6392, 0.6353, 0.6314,  ..., 0.5294, 0.5529, 0.5412],\n",
      "         [0.6275, 0.6275, 0.6392,  ..., 0.5412, 0.5608, 0.5922]],\n",
      "\n",
      "        [[0.4941, 0.5647, 0.7098,  ..., 0.4000, 0.5294, 0.5569],\n",
      "         [0.3529, 0.4314, 0.5529,  ..., 0.5961, 0.6196, 0.6039],\n",
      "         [0.3686, 0.3686, 0.4275,  ..., 0.6235, 0.6039, 0.6157],\n",
      "         ...,\n",
      "         [0.5961, 0.5686, 0.5647,  ..., 0.5647, 0.5804, 0.5608],\n",
      "         [0.6196, 0.6078, 0.5922,  ..., 0.5137, 0.5373, 0.5255],\n",
      "         [0.5961, 0.5922, 0.6000,  ..., 0.4745, 0.5137, 0.5294]],\n",
      "\n",
      "        [[0.5059, 0.5647, 0.6863,  ..., 0.3843, 0.4824, 0.5098],\n",
      "         [0.3686, 0.4549, 0.5373,  ..., 0.5059, 0.5176, 0.5020],\n",
      "         [0.3686, 0.3882, 0.4627,  ..., 0.5176, 0.5059, 0.5176],\n",
      "         ...,\n",
      "         [0.4510, 0.4196, 0.4078,  ..., 0.4314, 0.4471, 0.4078],\n",
      "         [0.4784, 0.4706, 0.4627,  ..., 0.3882, 0.4275, 0.4039],\n",
      "         [0.4784, 0.4706, 0.4706,  ..., 0.4039, 0.4196, 0.4588]]])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=32x32>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## convert image to tensor\n",
    "\n",
    "to_tensor = transforms.ToTensor ()\n",
    "\n",
    "img_train_tr = to_tensor ( img_train )\n",
    "\n",
    "print ( img_train_tr.shape)\n",
    "print ( img_train_tr)\n",
    "img_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8be708fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## get images from folder, not online. \n",
    "tensor_cifar10_training = datasets.CIFAR10(data_path, train = True, download = False, transform = transforms.ToTensor())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "94961f1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "img_tr, label = tensor_cifar10_training[23456]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b000353b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.7412, 0.6314, 0.6627,  ..., 0.5922, 0.8471, 0.9922],\n",
      "         [0.4588, 0.2392, 0.3020,  ..., 0.1647, 0.6431, 0.9608],\n",
      "         [0.4392, 0.2000, 0.2667,  ..., 0.1451, 0.5137, 0.8824],\n",
      "         ...,\n",
      "         [0.8392, 0.7725, 0.7882,  ..., 0.7255, 0.6196, 0.8118],\n",
      "         [0.8863, 0.7294, 0.6627,  ..., 0.6275, 0.6196, 0.8353],\n",
      "         [0.9412, 0.7608, 0.6627,  ..., 0.6588, 0.7216, 0.8902]],\n",
      "\n",
      "        [[0.7725, 0.6863, 0.7137,  ..., 0.5961, 0.8353, 0.9804],\n",
      "         [0.5137, 0.3412, 0.4235,  ..., 0.1843, 0.6353, 0.9490],\n",
      "         [0.5020, 0.3333, 0.4275,  ..., 0.1804, 0.5137, 0.8745],\n",
      "         ...,\n",
      "         [0.8235, 0.7490, 0.7725,  ..., 0.7255, 0.6196, 0.8196],\n",
      "         [0.8706, 0.6980, 0.6431,  ..., 0.6157, 0.6157, 0.8392],\n",
      "         [0.9333, 0.7451, 0.6549,  ..., 0.6549, 0.7176, 0.8902]],\n",
      "\n",
      "        [[0.7686, 0.6745, 0.6784,  ..., 0.5882, 0.8353, 0.9843],\n",
      "         [0.5020, 0.3020, 0.3373,  ..., 0.1843, 0.6431, 0.9569],\n",
      "         [0.4941, 0.2784, 0.3216,  ..., 0.1882, 0.5333, 0.8824],\n",
      "         ...,\n",
      "         [0.7765, 0.6706, 0.6980,  ..., 0.6471, 0.5882, 0.8118],\n",
      "         [0.8510, 0.6627, 0.6000,  ..., 0.5882, 0.6157, 0.8471],\n",
      "         [0.9294, 0.7333, 0.6392,  ..., 0.6510, 0.7294, 0.8980]]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print (img_tr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5050f41b",
   "metadata": {},
   "source": [
    "\n",
    "## Normalize\n",
    "* finding averages , averages of all values of green, red, blue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a31fa11b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## get images and label them\n",
    "\n",
    "imgs_list = [img_t for img_t, label in tensor_cifar10_training]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cf8998cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len (imgs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6ad6209b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 32, 32, 50000])\n",
      "tensor([[[[0.2314, 0.6039, 1.0000,  ..., 0.1373, 0.7412, 0.8980],\n",
      "          [0.1686, 0.4941, 0.9922,  ..., 0.1569, 0.7294, 0.9255],\n",
      "          [0.1961, 0.4118, 0.9922,  ..., 0.1647, 0.7255, 0.9176],\n",
      "          ...,\n",
      "          [0.6196, 0.3569, 0.9922,  ..., 0.3882, 0.6863, 0.8510],\n",
      "          [0.5961, 0.3412, 0.9922,  ..., 0.3098, 0.6745, 0.8667],\n",
      "          [0.5804, 0.3098, 0.9922,  ..., 0.3490, 0.6627, 0.8706]],\n",
      "\n",
      "         [[0.0627, 0.5490, 1.0000,  ..., 0.2235, 0.7608, 0.8706],\n",
      "          [0.0000, 0.5686, 1.0000,  ..., 0.1725, 0.7490, 0.9373],\n",
      "          [0.0706, 0.4902, 1.0000,  ..., 0.1961, 0.7451, 0.9137],\n",
      "          ...,\n",
      "          [0.4824, 0.3765, 1.0000,  ..., 0.6118, 0.6784, 0.8745],\n",
      "          [0.4667, 0.3020, 1.0000,  ..., 0.5529, 0.6706, 0.8902],\n",
      "          [0.4784, 0.2784, 1.0000,  ..., 0.4549, 0.6549, 0.8235]],\n",
      "\n",
      "         [[0.0980, 0.5490, 1.0000,  ..., 0.3843, 0.8157, 0.8353],\n",
      "          [0.0627, 0.5451, 0.9961,  ..., 0.2510, 0.8039, 0.9176],\n",
      "          [0.1922, 0.4510, 0.9961,  ..., 0.2706, 0.8000, 0.9059],\n",
      "          ...,\n",
      "          [0.4627, 0.3098, 0.9961,  ..., 0.7373, 0.6863, 0.8627],\n",
      "          [0.4706, 0.2667, 0.9961,  ..., 0.4667, 0.6745, 0.8627],\n",
      "          [0.4275, 0.2627, 0.9961,  ..., 0.2392, 0.6627, 0.7922]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.8157, 0.6863, 0.4431,  ..., 0.2863, 0.8118, 0.5882],\n",
      "          [0.7882, 0.6118, 0.4353,  ..., 0.2078, 0.7961, 0.5490],\n",
      "          [0.7765, 0.6039, 0.4118,  ..., 0.2118, 0.7961, 0.5176],\n",
      "          ...,\n",
      "          [0.6275, 0.1647, 0.2824,  ..., 0.0667, 0.5294, 0.8784],\n",
      "          [0.2196, 0.2392, 0.2824,  ..., 0.0824, 0.6353, 0.9020],\n",
      "          [0.2078, 0.3647, 0.2824,  ..., 0.1294, 0.6588, 0.9451]],\n",
      "\n",
      "         [[0.7059, 0.6471, 0.4353,  ..., 0.2392, 0.7765, 0.5373],\n",
      "          [0.6784, 0.6118, 0.4078,  ..., 0.2157, 0.7412, 0.5098],\n",
      "          [0.7294, 0.6235, 0.3882,  ..., 0.2235, 0.7059, 0.4902],\n",
      "          ...,\n",
      "          [0.7216, 0.4039, 0.2667,  ..., 0.0941, 0.6980, 0.7098],\n",
      "          [0.3804, 0.4824, 0.2745,  ..., 0.0667, 0.6863, 0.7922],\n",
      "          [0.3255, 0.5137, 0.3059,  ..., 0.0275, 0.6863, 0.8314]],\n",
      "\n",
      "         [[0.6941, 0.6392, 0.4157,  ..., 0.1725, 0.7765, 0.4784],\n",
      "          [0.6588, 0.6196, 0.3882,  ..., 0.1804, 0.7412, 0.4627],\n",
      "          [0.7020, 0.6392, 0.3725,  ..., 0.1922, 0.6980, 0.4706],\n",
      "          ...,\n",
      "          [0.8471, 0.5608, 0.3059,  ..., 0.1059, 0.7647, 0.7020],\n",
      "          [0.5922, 0.5608, 0.3098,  ..., 0.0824, 0.7686, 0.6431],\n",
      "          [0.4824, 0.5608, 0.3137,  ..., 0.0471, 0.7647, 0.6392]]],\n",
      "\n",
      "\n",
      "        [[[0.2431, 0.6941, 1.0000,  ..., 0.6980, 0.8275, 0.8980],\n",
      "          [0.1804, 0.5373, 0.9922,  ..., 0.6902, 0.8157, 0.9294],\n",
      "          [0.1882, 0.4078, 0.9922,  ..., 0.6902, 0.8118, 0.9255],\n",
      "          ...,\n",
      "          [0.5176, 0.3725, 0.9922,  ..., 0.6941, 0.7647, 0.8588],\n",
      "          [0.4902, 0.3529, 0.9922,  ..., 0.5765, 0.7608, 0.8745],\n",
      "          [0.4863, 0.3176, 0.9922,  ..., 0.5804, 0.7608, 0.8745]],\n",
      "\n",
      "         [[0.0784, 0.6275, 1.0000,  ..., 0.7137, 0.8235, 0.8667],\n",
      "          [0.0000, 0.6000, 1.0000,  ..., 0.7216, 0.8118, 0.9373],\n",
      "          [0.0314, 0.4902, 1.0000,  ..., 0.7176, 0.8078, 0.9176],\n",
      "          ...,\n",
      "          [0.3451, 0.3882, 1.0000,  ..., 0.7137, 0.7529, 0.8745],\n",
      "          [0.3255, 0.3137, 1.0000,  ..., 0.6941, 0.7490, 0.8941],\n",
      "          [0.3412, 0.2863, 1.0000,  ..., 0.5843, 0.7451, 0.8275]],\n",
      "\n",
      "         [[0.0941, 0.6078, 1.0000,  ..., 0.7725, 0.8588, 0.8078],\n",
      "          [0.0275, 0.5725, 0.9961,  ..., 0.7412, 0.8471, 0.9098],\n",
      "          [0.1059, 0.4510, 0.9961,  ..., 0.7529, 0.8431, 0.9137],\n",
      "          ...,\n",
      "          [0.3294, 0.3216, 0.9961,  ..., 0.7647, 0.7490, 0.8627],\n",
      "          [0.3294, 0.2745, 0.9961,  ..., 0.5294, 0.7451, 0.8588],\n",
      "          [0.2863, 0.2706, 0.9961,  ..., 0.3098, 0.7490, 0.7961]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.6667, 0.6549, 0.4706,  ..., 0.3098, 0.7804, 0.5608],\n",
      "          [0.6000, 0.6039, 0.4627,  ..., 0.2471, 0.7647, 0.5294],\n",
      "          [0.6314, 0.6275, 0.4392,  ..., 0.2667, 0.7686, 0.4980],\n",
      "          ...,\n",
      "          [0.5216, 0.1333, 0.3176,  ..., 0.1569, 0.5176, 0.8706],\n",
      "          [0.1216, 0.2078, 0.3137,  ..., 0.1412, 0.6196, 0.8941],\n",
      "          [0.1333, 0.3255, 0.3137,  ..., 0.1882, 0.6392, 0.9451]],\n",
      "\n",
      "         [[0.5451, 0.6039, 0.4627,  ..., 0.2667, 0.7451, 0.5176],\n",
      "          [0.4824, 0.5961, 0.4353,  ..., 0.2745, 0.7098, 0.4980],\n",
      "          [0.5647, 0.6314, 0.4157,  ..., 0.3098, 0.6745, 0.4745],\n",
      "          ...,\n",
      "          [0.5804, 0.3647, 0.2941,  ..., 0.1882, 0.6706, 0.7059],\n",
      "          [0.2431, 0.4471, 0.2980,  ..., 0.1373, 0.6627, 0.7882],\n",
      "          [0.2078, 0.4745, 0.3294,  ..., 0.0902, 0.6627, 0.8275]],\n",
      "\n",
      "         [[0.5647, 0.5804, 0.4431,  ..., 0.2196, 0.7412, 0.4667],\n",
      "          [0.5059, 0.5804, 0.4157,  ..., 0.2588, 0.7098, 0.4549],\n",
      "          [0.5569, 0.6118, 0.4000,  ..., 0.3020, 0.6667, 0.4549],\n",
      "          ...,\n",
      "          [0.7216, 0.5216, 0.3333,  ..., 0.2039, 0.7216, 0.6941],\n",
      "          [0.4627, 0.5255, 0.3333,  ..., 0.1686, 0.7412, 0.6431],\n",
      "          [0.3608, 0.5216, 0.3373,  ..., 0.1216, 0.7451, 0.6392]]],\n",
      "\n",
      "\n",
      "        [[[0.2471, 0.7333, 1.0000,  ..., 0.9216, 0.9412, 0.9373],\n",
      "          [0.1765, 0.5333, 0.9922,  ..., 0.9373, 0.9255, 0.9686],\n",
      "          [0.1686, 0.3725, 0.9922,  ..., 0.9451, 0.9216, 0.9686],\n",
      "          ...,\n",
      "          [0.4235, 0.2784, 0.9922,  ..., 0.8588, 0.8784, 0.9137],\n",
      "          [0.4000, 0.2784, 0.9922,  ..., 0.7725, 0.8706, 0.9176],\n",
      "          [0.4039, 0.2745, 0.9922,  ..., 0.7412, 0.8627, 0.9137]],\n",
      "\n",
      "         [[0.0784, 0.6627, 1.0000,  ..., 0.9176, 0.9373, 0.8980],\n",
      "          [0.0000, 0.6039, 1.0000,  ..., 0.9804, 0.9255, 0.9765],\n",
      "          [0.0000, 0.4627, 1.0000,  ..., 0.9412, 0.9216, 0.9647],\n",
      "          ...,\n",
      "          [0.2157, 0.3059, 1.0000,  ..., 0.7843, 0.8627, 0.9255],\n",
      "          [0.1961, 0.2431, 1.0000,  ..., 0.8078, 0.8549, 0.9333],\n",
      "          [0.2235, 0.2392, 1.0000,  ..., 0.6863, 0.8471, 0.8627]],\n",
      "\n",
      "         [[0.0824, 0.6431, 1.0000,  ..., 0.9294, 0.9569, 0.8275],\n",
      "          [0.0000, 0.5843, 0.9961,  ..., 0.9882, 0.9412, 0.9373],\n",
      "          [0.0314, 0.4392, 0.9961,  ..., 0.9608, 0.9373, 0.9569],\n",
      "          ...,\n",
      "          [0.1961, 0.2510, 0.9961,  ..., 0.8078, 0.8510, 0.9098],\n",
      "          [0.1961, 0.2157, 0.9961,  ..., 0.5765, 0.8471, 0.9098],\n",
      "          [0.1647, 0.2157, 0.9961,  ..., 0.3529, 0.8431, 0.8431]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.3765, 0.6510, 0.4392,  ..., 0.3020, 0.7098, 0.5294],\n",
      "          [0.1333, 0.6275, 0.4353,  ..., 0.2667, 0.6863, 0.4980],\n",
      "          [0.1020, 0.6667, 0.4157,  ..., 0.3137, 0.6784, 0.4706],\n",
      "          ...,\n",
      "          [0.2745, 0.1412, 0.3137,  ..., 0.2510, 0.4980, 0.8549],\n",
      "          [0.0275, 0.2235, 0.3098,  ..., 0.2000, 0.5882, 0.8824],\n",
      "          [0.0784, 0.3569, 0.3098,  ..., 0.1922, 0.5922, 0.9333]],\n",
      "\n",
      "         [[0.3765, 0.5020, 0.4314,  ..., 0.2941, 0.6667, 0.4941],\n",
      "          [0.1647, 0.5098, 0.4078,  ..., 0.3373, 0.6235, 0.4706],\n",
      "          [0.1176, 0.5569, 0.3843,  ..., 0.4039, 0.5765, 0.4510],\n",
      "          ...,\n",
      "          [0.3686, 0.3765, 0.2863,  ..., 0.2824, 0.6275, 0.6980],\n",
      "          [0.1333, 0.4706, 0.2941,  ..., 0.2078, 0.6118, 0.7765],\n",
      "          [0.1333, 0.5137, 0.3216,  ..., 0.1255, 0.6039, 0.8118]],\n",
      "\n",
      "         [[0.4549, 0.4706, 0.4118,  ..., 0.2863, 0.6784, 0.4471],\n",
      "          [0.3686, 0.4784, 0.3843,  ..., 0.3451, 0.6353, 0.4314],\n",
      "          [0.3412, 0.5216, 0.3686,  ..., 0.4118, 0.5843, 0.4353],\n",
      "          ...,\n",
      "          [0.5490, 0.5451, 0.3255,  ..., 0.3020, 0.6627, 0.6784],\n",
      "          [0.3294, 0.5569, 0.3255,  ..., 0.2588, 0.6706, 0.6353],\n",
      "          [0.2824, 0.5647, 0.3294,  ..., 0.1961, 0.6706, 0.6314]]]])\n"
     ]
    }
   ],
   "source": [
    "## stacking all images \n",
    "\n",
    "all_imgs_tr = torch.stack (imgs_list, dim = 3)\n",
    "\n",
    "print (all_imgs_tr.shape)\n",
    "print (all_imgs_tr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c9e6123e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 51200000])\n"
     ]
    }
   ],
   "source": [
    "## view1 = all_imgs_tr.view (3, -1, 50000) => torch.Size([3, 1024, 50000]) => 3 color , 32x32 = 1024 , 50,000 images. now all images into matrix (1024x50000).\n",
    "\n",
    "## view1 = all_imgs_tr.view (3, -1) => torch.Size([3, 51200000]) => 3 color , 1024x50,000. now all images into matrix (1x51'200'000).\n",
    "\n",
    "\n",
    "view1 = all_imgs_tr.view (3, -1)\n",
    "\n",
    "print (view1.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ffc1003a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4914, 0.4822, 0.4465])\n"
     ]
    }
   ],
   "source": [
    "## take the mean of them by each color\n",
    "\n",
    "view1_mean = view1.mean(dim=1)\n",
    "\n",
    "print (view1_mean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c1cc8007",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2470, 0.2435, 0.2616])\n"
     ]
    }
   ],
   "source": [
    "## finding standard deviation of means of colors\n",
    "\n",
    "view1_std = view1.std(dim = 1)\n",
    "\n",
    "print (view1_std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2768a3fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Now, use mean values (standard deviation) to replace values that in images\n",
    "## pixel = ( pixel - mean ) / standard_dev \n",
    "\n",
    "\n",
    "transformed_cifar10_train = datasets.CIFAR10 (data_path, train = True, download = False,\n",
    "                                                 transform = transforms.Compose (\n",
    "                                                     [\n",
    "                                                         transforms.ToTensor (),\n",
    "                                                         transforms.Normalize(view1_mean, view1_std)\n",
    "                                                         \n",
    "                                                     ]\n",
    "                                                 )\n",
    "                                             )\n",
    "\n",
    "transformed_cifar10_test = datasets.CIFAR10 (data_path, train = False, download = False,\n",
    "                                                 transform = transforms.Compose (\n",
    "                                                     [\n",
    "                                                         transforms.ToTensor (),\n",
    "                                                         transforms.Normalize(view1_mean, view1_std)\n",
    "                                                         \n",
    "                                                     ]\n",
    "                                                 )\n",
    "                                             )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5ae9078a-d071-4c24-934a-3457037d9a1c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 32, 32, 3)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "transformed_cifar10_test.data.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91529e3c-1b4f-4266-b5a7-9f5880742dd2",
   "metadata": {},
   "source": [
    "* 32 x 32 x 3 = 3072"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1428ed6c-1c64-40fe-955c-346241b3e473",
   "metadata": {},
   "source": [
    "## Build Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9904ebd0-c181-4dda-a9e7-ffb132b0f660",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## \n",
    "label_map = { 0:0, 2:1 }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8fd3e047",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## labeling\n",
    "\n",
    "cifar10_final_train = [ (img, label_map [label]) for img, label in transformed_cifar10_train if label in [0, 2] ] \n",
    "## choose only labeled 0 and 2, not all\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fd3cbaa0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "cifar10_final_test = [ (img, label_map [label]) for img, label in transformed_cifar10_test if label in [0, 2] ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9122254-df45-4b20-9a98-e238801c4f76",
   "metadata": {},
   "source": [
    "\n",
    "## Dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fc1bf954-2c97-47aa-bff6-e062ac72f446",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "34b12220",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "train_DataLoader = torch.utils.data.DataLoader (cifar10_final_train, batch_size = batch_size, shuffle = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48733a70-3222-455b-b602-082ddb63b7f3",
   "metadata": {},
   "source": [
    "\n",
    "## Architectures\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f026e2a6-7da1-436b-8311-5cdf5507fc83",
   "metadata": {},
   "source": [
    "* homework: we do it by object oriented way\n",
    "\n",
    "def mlp ()\n",
    "    init\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c423f357-66d6-4614-8c4a-aaa89ab60aab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## you need to conver these to object oriented format\n",
    "\n",
    "model_mlp = nn.Sequential(\n",
    "        nn.Linear (3072, 512), ## neurons less than input, like that funnel\n",
    "        nn.ReLU (), ## Tanh(), Sigmoid(), etc.\n",
    "        nn.Linear (512, 2), ## 2 vectors = 2 classification\n",
    "        nn.Softmax (dim = 1)\n",
    "\n",
    ")\n",
    "\n",
    "## 10 vectors we wanted\n",
    "## labeled 2 -> [0,0,1,0,0,0,0,0,0,0]\n",
    "## labeled 7 -> [0,0,0,0,0,0,0,1,0,0]\n",
    "\n",
    "## real result will be looks like\n",
    "## labeled 7 -> [0.2,0.5,0,0.4,0,0,0,0.9,0,0]\n",
    "\n",
    "## softmax can helps to solve this issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3ae024ee-143b-49c8-8807-add06d7bb274",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Deep Learning \n",
    "\n",
    "model_3DL = nn.Sequential(\n",
    "        nn.Linear (3072, 1024), \n",
    "        nn.ReLU (),\n",
    "        nn.Linear (1024, 512), \n",
    "        nn.ReLU (),\n",
    "        nn.Linear (512, 128), \n",
    "        nn.ReLU (),\n",
    "        nn.Linear (128, 2), \n",
    "        nn.LogSoftmax (dim = 1)\n",
    "\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f2673f83-8cab-49f9-8f87-62ac4b47002b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## in classification problem, use CrossEntropyLoss function for loss function\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss  ( )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bac44dd5-a7b0-43ff-a74d-b6edc55fe653",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "model_fn = model_3DL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df2a4ac-afca-4cf5-a31f-a6d975378dab",
   "metadata": {},
   "source": [
    "## Training Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7762fcab-3f87-4849-93bc-bdba8da8fda4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "n_epochs = 10\n",
    "\n",
    "optimizer = optim.Adam( model_fn.parameters(), lr = learning_rate )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b95831eb-f5e9-42e4-a20b-24d12d6b8a70",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3522, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3876, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2959, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4686, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3507, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1803, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2269, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1004, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.9968, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0906, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range (n_epochs):\n",
    "    for imgs, labels in train_DataLoader:\n",
    "        ## print (labels)\n",
    "        ## resize img\n",
    "        imgs_resized = imgs.view(imgs.shape[0], -1) ## -1 ???\n",
    "        preds = model_fn(imgs_resized) ## will return classification label\n",
    "        loss = loss_fn ( preds, labels )\n",
    "        \n",
    "        optimizer.zero_grad ()\n",
    "        loss.backward () ## compute new gratiante \n",
    "        optimizer.step ()\n",
    "    print (loss)\n",
    " \n",
    " ## tensor([6, 2, 3, 2, 8, 5, 6, 4, 7, 6, 7, 5, 9, 9, 5, 5, 5, 1, 3, 2, 4, 3, 0, 8,\n",
    " ##        2, 4, 6, 4, 0, 5, 7, 9])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddc2ed8-9309-4541-aa66-e9573bba1123",
   "metadata": {},
   "source": [
    "\n",
    "## Test model on the test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9f89dd49-e6fe-45da-a526-c9926167ee77",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len (cifar10_final_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f3160a1f-a7e1-4572-9e3c-dfd84ac044b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_DataLoader = torch.utils.data.DataLoader (cifar10_final_test, batch_size = 2000, shuffle = False )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "08358fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in test_DataLoader:\n",
    "        batch_size = imgs.shape [0]\n",
    "        outputs = model_fn ( imgs.view(batch_size, -1) )\n",
    "        val, indeces = torch.max (outputs, dim = 1)\n",
    "        preds = indeces \n",
    "        metrics = (preds == labels).sum()\n",
    "        \n",
    "        total = imgs.shape[1]\n",
    "        \n",
    "result = metrics / total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0fb2bfe4-3f6e-404c-a065-2da05b8a127b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(568.6667)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "78acd0d1-95c6-4261-b020-0d30d8b5e056",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def print_metrics_function (y_test, y_pred):\n",
    "    print ('Accuracy: %.2f' % accuracy_score (y_test, y_pred))\n",
    "    confmat = confusion_matrix (y_true = y_test, y_pred = y_pred)\n",
    "    print (\"Confusion Matrix:\")\n",
    "    print (confmat)\n",
    "    \n",
    "    print ('Precision: %.3f' % precision_score (y_true = y_test, y_pred = y_pred, average = 'weighted'))\n",
    "    print ('Recall: %.3f' % recall_score (y_true = y_test, y_pred = y_pred, average = 'weighted'))\n",
    "    print ('F1-measure: %.3f' % f1_score (y_true = y_test, y_pred = y_pred, average = 'weighted'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c947008c-5e82-412d-afd6-c6f9fc503fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.85\n",
      "Confusion Matrix:\n",
      "[[878 122]\n",
      " [172 828]]\n",
      "Precision: 0.854\n",
      "Recall: 0.853\n",
      "F1-measure: 0.853\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in test_DataLoader:\n",
    "        batch_size = imgs.shape [0]\n",
    "        outputs = model_fn ( imgs.view(batch_size, -1) )\n",
    "        val, indeces = torch.max (outputs, dim = 1)\n",
    "        preds = indeces \n",
    "        print_metrics_function (labels, preds)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb59fb56-fbad-4694-a613-85d315c39720",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e54a6a-3f04-4bc1-a13f-c241a69fe1b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975925b9-51de-427a-b4ca-78b0b7c0c26c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c764901f-c26d-420f-8e32-16aa27e50107",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47ee3aa-b922-4f8b-979f-718686a90113",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64c8d09-1cd6-429b-9e31-46c0ffdb53de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c7f4f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
